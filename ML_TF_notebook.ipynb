{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff69d918",
   "metadata": {},
   "source": [
    "# End-to-End TensorFlow Pipeline Notebook\n",
    "\n",
    "Dieses Notebook führt dich Schritt für Schritt durch einen kompletten Machine-Learning-Workflow mit TensorFlow:\n",
    "\n",
    "1. **Daten-Pipeline**: Laden, Vorverarbeiten und Batchen von Bildern mit `tf.data`.\n",
    "2. **TFRecord-Erstellung**: Konvertiere deine Bilder und Labels in das effiziente TFRecord-Format.\n",
    "3. **Basic CNN**: Implementiere ein einfaches Convolutional Neural Network (CNN) von Grund auf.\n",
    "4. **Pretrained ResNet**: Nutze ein ResNet-Modell mit vortrainierten Gewichten.\n",
    "5. **Modellspeicherung**: Speichere und lade dein trainiertes Modell.\n",
    "6. **Evaluation**: Beurteile die Modellleistung auf einem Evaluierungsdatensatz.\n",
    "\n",
    "Jede Zelle enthält ausführliche Erklärungen und Verweise auf die TensorFlow-Dokumentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910d6af7",
   "metadata": {},
   "source": [
    "## 1. Setup und Imports\n",
    "\n",
    "Wir starten mit den benötigten Bibliotheken. Stelle sicher, dass TensorFlow 2.x installiert ist.\n",
    "\n",
    "Referenzen:\n",
    "- TensorFlow Installationsanleitung: https://www.tensorflow.org/install\n",
    "- TensorFlow 2 Guide: https://www.tensorflow.org/guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3805b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c63ca3a",
   "metadata": {},
   "source": [
    "## 2. Daten-Pipeline erstellen\n",
    "\n",
    "Wir erstellen zwei separate Datensätze:\n",
    "- **Trainings-Datensatz**: Zum Trainieren unseres Modells\n",
    "- **Evaluierungs-Datensatz**: Zum Testen unseres Modells\n",
    "\n",
    "Die Pipeline lädt Bilder aus Ordnern, verkleinert sie auf eine einheitliche Größe und normalisiert die Pixelwerte.\n",
    "\n",
    "**Was passiert hier:**\n",
    "- Bilder werden geladen und in das richtige Format gebracht\n",
    "- Pixelwerte werden von 0-255 auf 0-1 normalisiert (das hilft beim Training)\n",
    "- Bilder werden in kleinere Gruppen (Batches) aufgeteilt\n",
    "\n",
    "**Wichtig:** Wir verwenden hier bewusst einfache Schleifen ohne parallele Verarbeitung, damit der Code leichter zu verstehen ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c124029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATEN-PIPELINE: TRAININGS- UND EVALUIERUNGS-DATENSÄTZE ERSTELLEN\n",
    "# =============================================================================\n",
    "\n",
    "# Pfade zu den beiden Datensatz-Ordnern\n",
    "train_data_dir = Path(\"./data/uni_test_train_ds\")\n",
    "eval_data_dir = Path(\"./data/uni_test_eval_ds\")\n",
    "\n",
    "# Parameter für die Bildverarbeitung\n",
    "batch_size = 4           # Anzahl Bilder pro Batch\n",
    "img_size = (256, 256)    # Alle Bilder werden auf diese Größe verkleinert\n",
    "\n",
    "# =============================================================================\n",
    "# HILFSFUNKTIONEN\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_process_image(image_path):\n",
    "    \"\"\"\n",
    "    Lädt ein PNG-Bild und bereitet es für das Training vor.\n",
    "    \"\"\"\n",
    "    # Bild von Festplatte lesen und dekodieren\n",
    "    image_raw = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image_raw, channels=3)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    \n",
    "    # Auf einheitliche Größe verkleinern und normalisieren\n",
    "    image = tf.image.resize(image, img_size)\n",
    "    image = image / 255.0  # Werte von 0-255 auf 0-1 bringen\n",
    "    \n",
    "    return image\n",
    "\n",
    "def load_labels_from_json(labels_file):\n",
    "    \"\"\"\n",
    "    Lädt die Labels aus einer Label Studio JSON-Datei.\n",
    "    \"\"\"\n",
    "    with open(labels_file, 'r') as f:\n",
    "        label_data = json.load(f)\n",
    "    \n",
    "    # Dictionary zum Zuordnen von Bildnamen zu Labels\n",
    "    image_to_label = {}\n",
    "    \n",
    "    for item in label_data:\n",
    "        # Bildpfad extrahieren\n",
    "        image_path = item['data']['image']\n",
    "        filename = image_path.split('/')[-1]  # Nur Dateiname\n",
    "        \n",
    "        # Label extrahieren\n",
    "        if item['annotations'] and len(item['annotations']) > 0:\n",
    "            annotation = item['annotations'][0]\n",
    "            if annotation['result'] and len(annotation['result']) > 0:\n",
    "                label = annotation['result'][0]['value']['choices'][0]\n",
    "                image_to_label[filename] = label\n",
    "    \n",
    "    return image_to_label\n",
    "\n",
    "def create_dataset_from_directory(data_dir):\n",
    "    \"\"\"\n",
    "    Erstellt einen TensorFlow-Datensatz aus PNG-Bildern und Label Studio Labels.\n",
    "    \"\"\"\n",
    "    print(f\"Erstelle Datensatz aus: {data_dir}\")\n",
    "    \n",
    "    # Labels aus JSON-Datei laden\n",
    "    labels_file = data_dir / \"labels\" / f\"labels_{data_dir.name}.json\"\n",
    "    image_to_label = load_labels_from_json(labels_file)\n",
    "    print(f\"Labels geladen: {len(image_to_label)} Einträge\")\n",
    "    \n",
    "    # Alle PNG-Bilder finden (der Ordner heißt 'images', nicht 'imgs')\n",
    "    image_dir = data_dir / \"images\"\n",
    "    image_paths = list(image_dir.glob(\"*.png\"))\n",
    "    print(f\"Gefundene PNG-Bilder: {len(image_paths)}\")\n",
    "    \n",
    "    if len(image_paths) == 0:\n",
    "        print(f\"WARNUNG: Keine PNG-Bilder in {image_dir} gefunden!\")\n",
    "        return None, None\n",
    "    \n",
    "    # Listen für Bilder und Labels erstellen\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Jedes Bild verarbeiten\n",
    "    for image_path in image_paths:\n",
    "        filename = image_path.name\n",
    "        \n",
    "        # Prüfen ob Label vorhanden ist\n",
    "        if filename in image_to_label:\n",
    "            # Bild laden\n",
    "            image = load_and_process_image(str(image_path))\n",
    "            images.append(image)\n",
    "            \n",
    "            # Label hinzufügen\n",
    "            label = image_to_label[filename]\n",
    "            labels.append(label)\n",
    "    \n",
    "    print(f\"Bilder mit Labels verarbeitet: {len(images)}\")\n",
    "    \n",
    "    if len(images) == 0:\n",
    "        print(\"FEHLER: Keine Bilder mit passenden Labels gefunden!\")\n",
    "        return None, None\n",
    "    \n",
    "    # In TensorFlow-Tensoren umwandeln\n",
    "    images_tensor = tf.stack(images)\n",
    "    \n",
    "    # Klassen sortieren und in Zahlen umwandeln\n",
    "    unique_classes = sorted(list(set(labels)))\n",
    "    print(f\"Klassen: {unique_classes}\")\n",
    "    \n",
    "    label_to_index = {name: i for i, name in enumerate(unique_classes)}\n",
    "    numeric_labels = [label_to_index[label] for label in labels]\n",
    "    labels_tensor = tf.constant(numeric_labels)\n",
    "    \n",
    "    # TensorFlow-Dataset erstellen und batchen\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images_tensor, labels_tensor))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset, unique_classes\n",
    "\n",
    "# =============================================================================\n",
    "# DATENSÄTZE ERSTELLEN\n",
    "# =============================================================================\n",
    "\n",
    "# Trainings-Datensatz\n",
    "print(\"🔄 Erstelle Trainings-Datensatz...\")\n",
    "train_dataset, train_classes = create_dataset_from_directory(train_data_dir)\n",
    "\n",
    "# Evaluierungs-Datensatz\n",
    "print(\"\\n🔄 Erstelle Evaluierungs-Dataset...\")\n",
    "eval_dataset, eval_classes = create_dataset_from_directory(eval_data_dir)\n",
    "\n",
    "# Klassennamen für später speichern und einmal zusammenfassen\n",
    "class_names = train_classes if train_classes else []\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Zusammenfassung der erstellten Datasets\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"📊 DATASET-ÜBERSICHT\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"✅ Trainings-Dataset: {len(train_classes) if train_classes else 0} Klassen\")\n",
    "print(f\"✅ Evaluierungs-Dataset: {len(eval_classes) if eval_classes else 0} Klassen\")\n",
    "print(f\"🏷️  Erkannte Klassen: {class_names}\")\n",
    "print(f\"🔢 Anzahl Klassen: {num_classes}\")\n",
    "\n",
    "# Prüfen ob beide Datasets die gleichen Klassen haben\n",
    "if train_classes and eval_classes:\n",
    "    if set(train_classes) == set(eval_classes):\n",
    "        print(\"✅ Beide Datasets haben identische Klassen - perfekt!\")\n",
    "    else:\n",
    "        print(\"⚠️  WARNUNG: Verschiedene Klassen in Train/Eval Datasets!\")\n",
    "        print(f\"   Train: {train_classes}\")\n",
    "        print(f\"   Eval:  {eval_classes}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f027eb",
   "metadata": {},
   "source": [
    "## 3. TFRecord-Datensatz erstellen\n",
    "\n",
    "Wir speichern die vorverarbeiteten Bilder und Labels im TFRecord-Format.\n",
    "\n",
    "- **`tf.train.Example`**: Struktur für einzelne Instanzen\n",
    "- **`tf.io.TFRecordWriter`**: Schreibt binäre Dateien\n",
    "\n",
    "Referenz:\n",
    "- TFRecord Guide: https://www.tensorflow.org/tutorials/load_data/tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f7547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TFRECORD-DATENSÄTZE ERSTELLEN\n",
    "# =============================================================================\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Hilfsfunktion für Bytes-Features.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Hilfsfunktion für Integer-Features.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def create_tfrecord(dataset, output_file, dataset_name):\n",
    "    \"\"\"\n",
    "    Erstellt eine TFRecord-Datei aus einem Dataset.\n",
    "    \"\"\"\n",
    "    print(f\"Erstelle {dataset_name} TFRecord: {output_file}\")\n",
    "    \n",
    "    with tf.io.TFRecordWriter(output_file) as writer:\n",
    "        for images, labels in dataset:\n",
    "            for img, lbl in zip(images, labels):\n",
    "                # Bild als PNG kodieren (besser für PNGs als JPEG)\n",
    "                img_raw = tf.io.encode_png(tf.cast(img * 255, tf.uint8)).numpy()\n",
    "                \n",
    "                # Feature erstellen\n",
    "                feature = {\n",
    "                    'image_raw': _bytes_feature(img_raw),\n",
    "                    'label': _int64_feature(int(lbl.numpy()))\n",
    "                }\n",
    "                \n",
    "                # Example erstellen und schreiben\n",
    "                example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "                writer.write(example.SerializeToString())\n",
    "    \n",
    "    print(f\"{dataset_name} TFRecord erstellt!\")\n",
    "\n",
    "# TFRecord-Dateien erstellen\n",
    "train_record_file = './data/train_dataset.tfrecord'\n",
    "eval_record_file = './data/eval_dataset.tfrecord'\n",
    "\n",
    "if train_dataset is not None:\n",
    "    create_tfrecord(train_dataset, train_record_file, \"Trainings\")\n",
    "\n",
    "if eval_dataset is not None:\n",
    "    create_tfrecord(eval_dataset, eval_record_file, \"Evaluierungs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10fe601",
   "metadata": {},
   "source": [
    "## 4. TFRecord-Dataset laden\n",
    "\n",
    "Wir laden den TFRecord-Datensatz und wandeln die `Example`-Protobufs zurück in Tensors um."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e012a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TFRECORD-DATASETS LADEN UND FÜR TRAINING VORBEREITEN\n",
    "# =============================================================================\n",
    "\n",
    "def parse_tfrecord_example(example_proto):\n",
    "    \"\"\"\n",
    "    Wandelt ein TFRecord-Example zurück in Bild und Label um.\n",
    "    \"\"\"\n",
    "    # Schema definieren\n",
    "    feature_description = {\n",
    "        'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    \n",
    "    # Example parsen\n",
    "    parsed = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    \n",
    "    # Bild dekodieren (PNG, da wir PNG kodiert haben)\n",
    "    image = tf.io.decode_png(parsed['image_raw'], channels=3)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.image.resize(image, img_size) / 255.0\n",
    "    \n",
    "    # Label in One-Hot-Encoding umwandeln\n",
    "    label = tf.one_hot(parsed['label'], depth=num_classes)\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "def load_and_split_tfrecord_dataset(record_file, dataset_name, train_split=0.8):\n",
    "    \"\"\"\n",
    "    Lädt ein TFRecord-Dataset und teilt es in Training und Validation auf.\n",
    "    \"\"\"\n",
    "    print(f\"Lade {dataset_name} TFRecord: {record_file}\")\n",
    "    \n",
    "    # TFRecord-Dataset laden\n",
    "    raw_dataset = tf.data.TFRecordDataset(record_file)\n",
    "    \n",
    "    # Examples parsen\n",
    "    parsed_dataset = raw_dataset.map(parse_tfrecord_example)\n",
    "    \n",
    "    # Dataset-Größe ermitteln (ungefähr)\n",
    "    dataset_size = sum(1 for _ in parsed_dataset)\n",
    "    print(f\"Dataset-Größe: {dataset_size} Beispiele\")\n",
    "    \n",
    "    # Dataset shuffeln für bessere Aufteilung\n",
    "    shuffled_dataset = parsed_dataset.shuffle(buffer_size=dataset_size, seed=42)\n",
    "    \n",
    "    # Train/Validation Split\n",
    "    train_size = int(dataset_size * train_split)\n",
    "    val_size = dataset_size - train_size\n",
    "    \n",
    "    train_dataset = shuffled_dataset.take(train_size)\n",
    "    val_dataset = shuffled_dataset.skip(train_size)\n",
    "    \n",
    "    # Batchen\n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "    val_dataset = val_dataset.batch(batch_size)\n",
    "    \n",
    "    print(f\"Training: {train_size} Beispiele ({train_split*100:.0f}%)\")\n",
    "    print(f\"Validation: {val_size} Beispiele ({(1-train_split)*100:.0f}%)\")\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "def load_tfrecord_dataset(record_file, dataset_name):\n",
    "    \"\"\"\n",
    "    Lädt ein TFRecord-Dataset für Evaluation (ohne Split).\n",
    "    \"\"\"\n",
    "    print(f\"Lade {dataset_name} TFRecord: {record_file}\")\n",
    "    \n",
    "    # TFRecord-Dataset laden\n",
    "    raw_dataset = tf.data.TFRecordDataset(record_file)\n",
    "    \n",
    "    # Examples parsen\n",
    "    parsed_dataset = raw_dataset.map(parse_tfrecord_example)\n",
    "    \n",
    "    # Batchen\n",
    "    batched_dataset = parsed_dataset.batch(batch_size)\n",
    "    \n",
    "    return batched_dataset\n",
    "\n",
    "# =============================================================================\n",
    "# DATASETS LADEN UND AUFTEILEN\n",
    "# =============================================================================\n",
    "\n",
    "# Trainings-TFRecord laden und in Train/Validation aufteilen\n",
    "train_tfrecord_dataset = None\n",
    "val_tfrecord_dataset = None\n",
    "eval_tfrecord_dataset = None\n",
    "\n",
    "if os.path.exists(train_record_file):\n",
    "    print(\"📊 Lade und teile Trainings-Dataset auf...\")\n",
    "    train_tfrecord_dataset, val_tfrecord_dataset = load_and_split_tfrecord_dataset(\n",
    "        train_record_file, \"Trainings\", train_split=0.8\n",
    "    )\n",
    "\n",
    "# Evaluierungs-TFRecord separat laden (wird nicht zum Training verwendet)\n",
    "if os.path.exists(eval_record_file):\n",
    "    print(\"\\n📊 Lade Evaluierungs-Dataset...\")\n",
    "    eval_tfrecord_dataset = load_tfrecord_dataset(eval_record_file, \"Evaluierungs\")\n",
    "\n",
    "# Zusammenfassung der verfügbaren Datasets\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"📊 VERFÜGBARE DATASETS FÜR TRAINING\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"✅ Training-Dataset: {train_tfrecord_dataset}\")\n",
    "print(f\"✅ Validation-Dataset: {val_tfrecord_dataset}\")\n",
    "print(f\"📋 Evaluation-Dataset: {eval_tfrecord_dataset} (nur für finale Evaluation)\")\n",
    "print(f\"🔢 Anzahl Klassen: {num_classes}\")\n",
    "print(f\"🏷️  Klassennamen: {class_names}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0ff924",
   "metadata": {},
   "source": [
    "## 5. Basic CNN erstellen und trainieren\n",
    "\n",
    "Ein einfaches CNN zum Einstieg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bd7487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BASIC CNN ERSTELLEN UND TRAINIEREN\n",
    "# =============================================================================\n",
    "\n",
    "# Einfaches CNN-Modell erstellen\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(*img_size, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Modell kompilieren\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Basic CNN Modell-Architektur:\")\n",
    "model.summary()\n",
    "\n",
    "# Training mit Train/Validation Split (80/20)\n",
    "epochs = 5\n",
    "\n",
    "if train_tfrecord_dataset is not None and val_tfrecord_dataset is not None:\n",
    "    print(f\"\\n🚀 Trainiere Basic CNN für {epochs} Epochen...\")\n",
    "    print(\"📊 Training: 80% der Daten\")\n",
    "    print(\"📊 Validation: 20% der Daten\")\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_tfrecord_dataset,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_tfrecord_dataset,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Training abgeschlossen!\")\n",
    "else:\n",
    "    print(\"❌ Kein Trainings-Dataset verfügbar. Überspringe Training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a67b0b",
   "metadata": {},
   "source": [
    "## 6. Pretrained ResNet nutzen\n",
    "\n",
    "Wir verwenden `tf.keras.applications.ResNet50` mit vortrainierten ImageNet-Gewichten.\n",
    "\n",
    "Referenz:\n",
    "- Keras Applications: https://www.tensorflow.org/api_docs/python/tf/keras/applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f36a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PRETRAINED RESNET NUTZEN\n",
    "# =============================================================================\n",
    "\n",
    "# ResNet50 Basis-Modell laden (ohne Top-Layer)\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    input_shape=(*img_size, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "# Basis-Modell einfrieren (Transfer Learning)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Eigenes Top-Layer hinzufügen\n",
    "inputs = tf.keras.Input(shape=(*img_size, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "resnet_model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Modell kompilieren\n",
    "resnet_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"ResNet50 Transfer Learning Modell-Architektur:\")\n",
    "resnet_model.summary()\n",
    "\n",
    "# Training mit Train/Validation Split (80/20)\n",
    "if train_tfrecord_dataset is not None and val_tfrecord_dataset is not None:\n",
    "    print(f\"\\n🚀 Trainiere ResNet50 für 3 Epochen...\")\n",
    "    print(\"📊 Training: 80% der Daten\")\n",
    "    print(\"📊 Validation: 20% der Daten\")\n",
    "    \n",
    "    resnet_history = resnet_model.fit(\n",
    "        train_tfrecord_dataset,\n",
    "        epochs=3,\n",
    "        validation_data=val_tfrecord_dataset,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"✅ ResNet Training abgeschlossen!\")\n",
    "else:\n",
    "    print(\"❌ Kein Trainings-Dataset verfügbar. Überspringe Training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c66f42f",
   "metadata": {},
   "source": [
    "## 7. Modell speichern und laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bf2a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODELL SPEICHERN UND LADEN\n",
    "# =============================================================================\n",
    "\n",
    "# Ordner für gespeicherte Modelle erstellen\n",
    "os.makedirs('./saved_models', exist_ok=True)\n",
    "\n",
    "# Basic CNN Modell speichern\n",
    "cnn_model_path = './saved_models/basic_cnn.keras'\n",
    "model.save(cnn_model_path)\n",
    "print(f\"Basic CNN Modell gespeichert unter: {cnn_model_path}\")\n",
    "\n",
    "# ResNet Modell speichern (falls trainiert)\n",
    "if 'resnet_model' in locals():\n",
    "    resnet_model_path = './saved_models/resnet_model.keras'\n",
    "    resnet_model.save(resnet_model_path)\n",
    "    print(f\"ResNet Modell gespeichert unter: {resnet_model_path}\")\n",
    "\n",
    "# Modell wieder laden (Beispiel)\n",
    "print(\"\\nLade gespeichertes Modell...\")\n",
    "loaded_model = tf.keras.models.load_model(cnn_model_path)\n",
    "print(\"Modell erfolgreich geladen!\")\n",
    "\n",
    "# Kurze Übersicht des geladenen Modells\n",
    "print(\"\\nÜbersicht des geladenen Modells:\")\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd3156",
   "metadata": {},
   "source": [
    "## 8. Ausführliche Modell-Evaluation\n",
    "\n",
    "Wir evaluieren beide Modelle (Basic CNN und ResNet) ausführlich mit verschiedenen TensorFlow-Metriken:\n",
    "\n",
    "- **Accuracy**: Wie oft liegt das Modell richtig?\n",
    "- **Loss**: Wie sicher ist sich das Modell bei seinen Vorhersagen?\n",
    "- **Confusion Matrix**: Welche Klassen werden verwechselt?\n",
    "- **Classification Report**: Detaillierte Metriken pro Klasse\n",
    "\n",
    "Referenzen:\n",
    "- TensorFlow Metrics: https://www.tensorflow.org/api_docs/python/tf/keras/metrics\n",
    "- Model Evaluation: https://www.tensorflow.org/guide/keras/evaluate_and_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020ada48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EINFACHE MODELL-EVALUATION MIT TENSORFLOW\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_model_simple(model, dataset, model_name):\n",
    "    \"\"\"\n",
    "    Führt eine einfache Evaluation eines Modells durch - nur mit TensorFlow.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EVALUATION: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if dataset is None:\n",
    "        print(\"Kein Evaluierungs-Dataset verfügbar!\")\n",
    "        return None\n",
    "    \n",
    "    # 1. Standard TensorFlow Evaluation\n",
    "    print(\"\\n1. Standard Metriken (TensorFlow):\")\n",
    "    eval_results = model.evaluate(dataset, verbose=1)\n",
    "    loss, accuracy = eval_results[0], eval_results[1]\n",
    "    print(f\"   • Test Loss: {loss:.4f}\")\n",
    "    print(f\"   • Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    \n",
    "    # 2. Vorhersagen sammeln\n",
    "    print(\"\\n2. Sammle Vorhersagen...\")\n",
    "    y_pred_probs = model.predict(dataset, verbose=0)\n",
    "    y_pred_classes = tf.argmax(y_pred_probs, axis=1).numpy()\n",
    "    \n",
    "    # Wahre Labels sammeln\n",
    "    y_true = []\n",
    "    for _, labels in dataset:\n",
    "        y_true.extend(tf.argmax(labels, axis=1).numpy())\n",
    "    y_true = tf.constant(y_true)\n",
    "    \n",
    "    # 3. Einfache Metriken mit TensorFlow\n",
    "    print(\"\\n3. Detaillierte Metriken:\")\n",
    "    \n",
    "    # Accuracy nochmal berechnen (zur Kontrolle)\n",
    "    correct_predictions = tf.cast(tf.equal(y_pred_classes, y_true), tf.float32)\n",
    "    manual_accuracy = tf.reduce_mean(correct_predictions).numpy()\n",
    "    print(f\"   • Manuelle Accuracy: {manual_accuracy:.4f}\")\n",
    "    \n",
    "    # Konfidenz-Analyse\n",
    "    max_probs = tf.reduce_max(y_pred_probs, axis=1)\n",
    "    print(f\"   • Durchschnittliche Konfidenz: {tf.reduce_mean(max_probs).numpy():.4f}\")\n",
    "    print(f\"   • Minimale Konfidenz: {tf.reduce_min(max_probs).numpy():.4f}\")\n",
    "    print(f\"   • Maximale Konfidenz: {tf.reduce_max(max_probs).numpy():.4f}\")\n",
    "    \n",
    "    # 4. Einfache Confusion Matrix (ohne externe Bibliotheken)\n",
    "    print(\"\\n4. Einfache Klassenverteilung:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        # Wie oft wurde diese Klasse vorhergesagt?\n",
    "        predicted_as_class = tf.reduce_sum(tf.cast(tf.equal(y_pred_classes, i), tf.int32)).numpy()\n",
    "        # Wie oft war diese Klasse tatsächlich richtig?\n",
    "        actually_class = tf.reduce_sum(tf.cast(tf.equal(y_true, i), tf.int32)).numpy()\n",
    "        print(f\"   • {class_name}: {actually_class} tatsächlich, {predicted_as_class} vorhergesagt\")\n",
    "    \n",
    "    # 5. Pro-Klassen Accuracy\n",
    "    print(\"\\n5. Accuracy pro Klasse:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        # Masken für diese Klasse\n",
    "        class_mask = tf.equal(y_true, i)\n",
    "        if tf.reduce_sum(tf.cast(class_mask, tf.int32)) > 0:  # Wenn Klasse existiert\n",
    "            class_predictions = tf.boolean_mask(y_pred_classes, class_mask)\n",
    "            class_accuracy = tf.reduce_mean(\n",
    "                tf.cast(tf.equal(class_predictions, i), tf.float32)\n",
    "            ).numpy()\n",
    "            print(f\"   • {class_name}: {class_accuracy:.4f} ({class_accuracy*100:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "        'y_true': y_true.numpy(),\n",
    "        'y_pred': y_pred_classes\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# MODELL-EVALUATIONEN DURCHFÜHREN\n",
    "# =============================================================================\n",
    "\n",
    "print(\"🔍 Starte Modell-Evaluation...\")\n",
    "\n",
    "# Basic CNN evaluieren\n",
    "print(\"\\n🤖 Evaluiere Basic CNN...\")\n",
    "cnn_results = evaluate_model_simple(model, eval_tfrecord_dataset, \"Basic CNN\")\n",
    "\n",
    "# ResNet evaluieren (falls vorhanden)\n",
    "resnet_results = None\n",
    "if 'resnet_model' in locals():\n",
    "    print(\"\\n🧠 Evaluiere ResNet50...\")\n",
    "    resnet_results = evaluate_model_simple(resnet_model, eval_tfrecord_dataset, \"ResNet50 Transfer Learning\")\n",
    "\n",
    "# =============================================================================\n",
    "# EINFACHER MODELL-VERGLEICH\n",
    "# =============================================================================\n",
    "\n",
    "if cnn_results and resnet_results:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"MODELL-VERGLEICH\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"{'Modell':<20} {'Accuracy':<12} {'Loss':<12}\")\n",
    "    print(\"-\" * 45)\n",
    "    print(f\"{'ResNet50':<20} {resnet_results['accuracy']:<12.4f} {resnet_results['loss']:<12.4f}\")\n",
    "    \n",
    "    # Einfache Empfehlung\n",
    "    if resnet_results['accuracy'] > cnn_results['accuracy']:\n",
    "        diff = (resnet_results['accuracy'] - cnn_results['accuracy']) * 100\n",
    "        print(f\"\\n🏆 GEWINNER: ResNet50 (+{diff:.1f}% besser)\")\n",
    "    elif cnn_results['accuracy'] > resnet_results['accuracy']:\n",
    "        diff = (cnn_results['accuracy'] - resnet_results['accuracy']) * 100\n",
    "        print(f\"\\n🏆 GEWINNER: Basic CNN (+{diff:.1f}% besser)\")\n",
    "    else:\n",
    "        print(f\"\\n🤝 UNENTSCHIEDEN: Beide Modelle gleich gut\")\n",
    "\n",
    "elif cnn_results:\n",
    "    print(f\"\\n✅ Basic CNN Evaluation abgeschlossen!\")\n",
    "    print(f\"   Accuracy: {cnn_results['accuracy']:.4f} ({cnn_results['accuracy']*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EVALUATION ABGESCHLOSSEN\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"🎯 Beide Modelle wurden erfolgreich auf dem Evaluierungs-Dataset getestet!\")\n",
    "print(\"📊 Die Ergebnisse zeigen, wie gut die Modelle auf ungesehenen Daten funktionieren.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cab2f4e",
   "metadata": {},
   "source": [
    "## 9. Fazit und Ausblick\n",
    "\n",
    "### Was du gelernt hast:\n",
    "\n",
    "1. **Daten-Pipeline**: Effizientes Laden und Vorverarbeiten von Bilddaten mit Label Studio JSON-Labels\n",
    "2. **TFRecord-Format**: Optimierte Datenspeicherung für große Datasets\n",
    "3. **CNN von Grund auf**: Aufbau eines einfachen Convolutional Neural Networks\n",
    "4. **Transfer Learning**: Nutzung vortrainierter ResNet50-Gewichte\n",
    "5. **Modellspeicherung**: Persistierung trainierter Modelle\n",
    "6. **Umfassende Evaluation**: Professionelle Bewertung mit TensorFlow-Metriken\n",
    "\n",
    "### Wichtige Erkenntnisse:\n",
    "\n",
    "- **TFRecord** ermöglicht effizientes Streaming großer Datasets\n",
    "- **Transfer Learning** kann deutlich bessere Ergebnisse erzielen als Training von Grund auf\n",
    "- **Evaluation** sollte immer mehrere Metriken und Visualisierungen umfassen\n",
    "- **Einfacher Code** ist oft besser als komplizierte Parallelverarbeitung (für Lernzwecke)\n",
    "\n",
    "### Nächste Schritte:\n",
    "\n",
    "- **Data Augmentation**: Erweitere den Datensatz künstlich\n",
    "- **Hyperparameter Tuning**: Optimiere Lernrate, Batch-Größe, etc.\n",
    "- **Andere Architekturen**: Probiere EfficientNet, Vision Transformer\n",
    "\n",
    "---\n",
    "\n",
    "### 📚 Weiterführende Ressourcen:\n",
    "\n",
    "**TensorFlow Grundlagen:**\n",
    "- [TensorFlow Guide](https://www.tensorflow.org/guide)\n",
    "- [Keras API](https://www.tensorflow.org/api_docs/python/tf/keras)\n",
    "\n",
    "**Daten-Pipeline:**\n",
    "- [tf.data Guide](https://www.tensorflow.org/guide/data)\n",
    "- [TFRecord Tutorial](https://www.tensorflow.org/tutorials/load_data/tfrecord)\n",
    "\n",
    "**Computer Vision:**\n",
    "- [Image Classification Tutorial](https://www.tensorflow.org/tutorials/images/classification)\n",
    "- [Transfer Learning Guide](https://www.tensorflow.org/tutorials/images/transfer_learning)\n",
    "\n",
    "**Evaluation & Metriken:**\n",
    "- [Model Evaluation](https://www.tensorflow.org/guide/keras/evaluate_and_predict)\n",
    "- [TensorFlow Metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)\n",
    "\n",
    "**Production Deployment:**\n",
    "- [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)\n",
    "- [TensorFlow Lite](https://www.tensorflow.org/lite)\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Tipps für eigene Projekte:\n",
    "\n",
    "1. **Starte einfach**: Verwende erst kleine Datasets und einfache Modelle\n",
    "2. **Visualisiere alles**: Confusion Matrix, Trainingsverläufe, Datenverteilung\n",
    "3. **Dokumentiere**: Notiere Hyperparameter und Ergebnisse\n",
    "4. **Validiere richtig**: Trenne Training, Validation und Evaluierung strikt\n",
    "5. **Iteriere**: Verbessere schrittweise statt alles auf einmal zu ändern"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
